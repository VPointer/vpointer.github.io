<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="Y0-gFMBzGnbrueUrh8PjkmnvCGItjob2oR3HjG9SVnE">
  <meta name="msvalidate.01" content="97A49017D4D536B99438C4EE0E9FBA3F">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.zuiqiangiron.xyz","root":"/","scheme":"Gemini","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="《R语言实战》笔记 本篇主要介绍普通最小二乘(ordinary least squares，OLS)回归法，包括简单线性回归、多项式回归和多元线性回归">
<meta property="og:type" content="article">
<meta property="og:title" content="R语言回归分析">
<meta property="og:url" content="https://www.zuiqiangiron.xyz/2020/04/25/R%E8%AF%AD%E8%A8%80%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/index.html">
<meta property="og:site_name" content="嘴强黑铁">
<meta property="og:description" content="《R语言实战》笔记 本篇主要介绍普通最小二乘(ordinary least squares，OLS)回归法，包括简单线性回归、多项式回归和多元线性回归">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://res.cloudinary.com/vpointer/image/upload/v1587987664/R/OSL%E5%9B%9E%E5%BD%92.svg">
<meta property="og:image" content="https://res.cloudinary.com/vpointer/image/upload/v1587876004/R/lm%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%AC%A6%E5%8F%B7.png">
<meta property="og:image" content="https://res.cloudinary.com/vpointer/image/upload/v1587876189/R/lm%E6%8B%9F%E5%90%88%E5%88%86%E6%9E%90%E5%87%BD%E6%95%B0.png">
<meta property="og:image" content="https://res.cloudinary.com/vpointer/image/upload/v1587878880/R/%E7%AE%80%E5%8D%95%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.png">
<meta property="og:image" content="https://res.cloudinary.com/vpointer/image/upload/v1587879332/R/%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92.png">
<meta property="og:image" content="https://res.cloudinary.com/vpointer/image/upload/v1587883551/R/scatterplot.png">
<meta property="og:image" content="https://res.cloudinary.com/vpointer/image/upload/v1587884604/R/scatterplotMatrix.png">
<meta property="og:image" content="https://res.cloudinary.com/vpointer/image/upload/v1587885966/R/lmplot.png">
<meta property="og:image" content="https://res.cloudinary.com/vpointer/image/upload/v1587889862/R/qqplotcar.png">
<meta property="og:image" content="https://res.cloudinary.com/vpointer/image/upload/v1587893311/R/crplot.png">
<meta property="og:image" content="https://res.cloudinary.com/vpointer/image/upload/v1587895945/R/spreadLevelPlot.png">
<meta property="og:image" content="https://res.cloudinary.com/vpointer/image/upload/v1587913038/R/hatvalues.png">
<meta property="og:image" content="https://res.cloudinary.com/vpointer/image/upload/v1587914726/R/cookd.png">
<meta property="og:image" content="https://res.cloudinary.com/vpointer/image/upload/v1587914985/R/avplots.png">
<meta property="og:image" content="https://res.cloudinary.com/vpointer/image/upload/v1587915753/R/influence.png">
<meta property="og:image" content="https://res.cloudinary.com/vpointer/image/upload/v1587959596/R/y%E5%8F%98%E6%8D%A2.png">
<meta property="og:image" content="https://res.cloudinary.com/vpointer/image/upload/v1587966028/R/regsubsets.png">
<meta property="article:published_time" content="2020-04-25T15:10:53.000Z">
<meta property="article:modified_time" content="2020-04-27T15:46:07.274Z">
<meta property="article:author" content="嘴强黑铁">
<meta property="article:tag" content="R语言">
<meta property="article:tag" content="R语言回归分析">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://res.cloudinary.com/vpointer/image/upload/v1587987664/R/OSL%E5%9B%9E%E5%BD%92.svg">

<link rel="canonical" href="https://www.zuiqiangiron.xyz/2020/04/25/R%E8%AF%AD%E8%A8%80%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>R语言回归分析 | 嘴强黑铁</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-161480657-1"></script>
    <script pjax>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-161480657-1');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">嘴强黑铁</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">天下事以难而废者十之一，以惰而废者十之九</h1>
      
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="search-pop-overlay">
  <div class="popup search-popup">
      <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

  </div>
</div>

  </div>
</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.zuiqiangiron.xyz/2020/04/25/R%E8%AF%AD%E8%A8%80%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/touxiang.jpg">
      <meta itemprop="name" content="嘴强黑铁">
      <meta itemprop="description" content="一顿操作猛如虎">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嘴强黑铁">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          R语言回归分析
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-25 23:10:53" itemprop="dateCreated datePublished" datetime="2020-04-25T23:10:53+08:00">2020-04-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-27 23:46:07" itemprop="dateModified" datetime="2020-04-27T23:46:07+08:00">2020-04-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/R%E8%AF%AD%E8%A8%80/" itemprop="url" rel="index"><span itemprop="name">R语言</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2020/04/25/R%E8%AF%AD%E8%A8%80%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/04/25/R语言回归分析/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<p>《R语言实战》笔记</p>
<p>本篇主要介绍普通最小二乘(<em>ordinary least squares</em>，<em>OLS</em>)回归法，包括简单线性回归、多项式回归和多元线性回归</p>
</blockquote>
<a id="more"></a>
<h1 id="思维导图">思维导图</h1>
<p><img src="https://res.cloudinary.com/vpointer/image/upload/v1587987664/R/OSL回归.svg" /></p>
<h1 id="osl回归">OSL回归</h1>
<p>OLS回归是通过预测变量的加权和来预测量化的因变量，其中权重是通过数据估计而得的参数。 OLS回归拟合模型的形式如下： <span class="math display">\[
\hat{Y}_{i}=\hat{\beta}_{0}+\hat{\beta}_{1} X_{1 i}+\cdots+\hat{\beta}_{k} X_{k i} \quad i=1 \cdots n
\]</span> 其中:</p>
<ol type="1">
<li><code>n</code>为观测的数目，<code>k</code>为预测变量的数目；</li>
<li><span class="math inline">\(\hat{Y}_{i}\)</span>：第<code>i</code>次观测对应的因变量的预测值(具体来讲，它是在已知预测变量值的条件下，对<code>Y</code>分布估计的均值)；</li>
<li><span class="math inline">\(X_{j i}\)</span>：第<code>i</code>次观测对应的第<code>j</code>个预测变量值；</li>
<li><span class="math inline">\(\hat{\beta}_{0}\)</span>：截距项(当所有的预测变量都为0时，<code>Y</code>的预测值)；</li>
<li><span class="math inline">\(\hat{\beta}_{j}\)</span>：预测变量<code>j</code>的回归系数(斜率表示<code>X</code>改变一个单位所引起的<code>Y</code>的改变量)。</li>
</ol>
<p>模型的目的是<strong>通过减少响应变量的真实值与预测值的差值来获得模型参数(截距项和斜率)。</strong> 具体而言，即<strong>使得残差平方和最小</strong>： <span class="math display">\[
\sum_{i=1}^{n}\left(Y_{i}-\hat{Y}_{i}\right)^{2}=\sum_{i=1}^{n}\left(Y_{i}-\hat{\beta}_{0}+\hat{\beta}_{1} X_{1 i}+\cdots+\hat{\beta}_{k} X_{k i}\right)^{2}=\sum_{i=1}^{n} \varepsilon_{i}^{2}
\]</span> 对于OSL模型来说，<strong>数据需要满足如下统计假设</strong>：</p>
<ol type="1">
<li><strong>正态性</strong>：对于固定的自变量值，因变量值成正态分布；</li>
<li><strong>独立性</strong>：<span class="math inline">\(Y_{i}\)</span>值之间相互独立；</li>
<li><strong>线性</strong>：因变量与自变量之间为线性相关；</li>
<li><strong>同方差性（不变方差）</strong>：因变量的方差不随自变量的水平不同而变化；</li>
</ol>
<p>OLS回归还假定自变量是固定的且测量无误差，但在实践中通常都放松了这个假设。</p>
<h2 id="几种常用的osl回归模型">几种常用的OSL回归模型</h2>
<ul>
<li><strong>简单线性回归</strong>：当回归模型包含一个因变量和一个自变量时；</li>
<li><strong>多项式回归</strong>：当只有一个预测变量，但同时包含变量的幂时（比如<span class="math inline">\(X\)</span>，<span class="math inline">\(X^{2}\)</span>，<span class="math inline">\(X^{3}\)</span>）；</li>
<li><strong>多元线性回归</strong>：自变量不止一个。</li>
</ul>
<h3 id="线性模型与非线性模型">线性模型与非线性模型</h3>
<p>多项式等式仍可认为是线性回归模型，因为等式仍是预测变量的加权和形式。即使这样的模型： <span class="math display">\[
\hat{Y}_{i}=\hat{\beta}_{0} \times \log X_{1}+\hat{\beta}_{2} \times \sin X_{2}
\]</span> 仍可认为是线性模型(参数项是线性的)。相反，下面的例子才能算是真正的非线性模型： <span class="math display">\[
\hat{Y}_{i}=\hat{\beta}_{0}+\hat{\beta}_{1} \mathrm{e}^{x / \beta_{2}}
\]</span></p>
<h2 id="lm函数拟合回归模型">lm()函数拟合回归模型</h2>
<p><strong>调用形式</strong>：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">myfit &lt;- lm(formula, data)</span><br></pre></td></tr></table></figure>
<p>其中，<code>formula</code>指要拟合的模型形式，<code>data</code>是一个数据框，包含了用于拟合模型的数据。结果对象存储在一个列表中，包含了所拟合模型的大量信息。</p>
<p><strong><code>formula</code>的形式如下</strong>：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Y ~ X1 + X2 + <span class="keyword">...</span> + Xk</span><br></pre></td></tr></table></figure>
<p><code>~</code>左边为响应变量，右边为各个预测变量，更多形式如下表所示：</p>
<p><img src="https://res.cloudinary.com/vpointer/image/upload/v1587876004/R/lm中常用符号.png" /></p>
<p><strong>对拟合出的结果，可用以下函数分析</strong>：</p>
<p><img src="https://res.cloudinary.com/vpointer/image/upload/v1587876189/R/lm拟合分析函数.png" /></p>
<h3 id="简单线性回归">简单线性回归</h3>
<p>以下是一个简单线性回归的示例：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&gt; fit &lt;- lm(weight ~ height, data=women)</span><br><span class="line">&gt; summary(fit)</span><br><span class="line"></span><br><span class="line">Call:</span><br><span class="line">lm(formula = weight ~ height, data = women)</span><br><span class="line"></span><br><span class="line">Residuals:</span><br><span class="line">    Min      1Q  Median      3Q     Max </span><br><span class="line">-<span class="number">1.7333</span> -<span class="number">1.1333</span> -<span class="number">0.3833</span>  <span class="number">0.7417</span>  <span class="number">3.1167</span> </span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">             Estimate Std. Error t value Pr(&gt;|t|)    </span><br><span class="line">(Intercept) -<span class="number">87.51667</span>    <span class="number">5.93694</span>  -<span class="number">14.74</span> <span class="number">1.71e-09</span> ***</span><br><span class="line">height        <span class="number">3.45000</span>    <span class="number">0.09114</span>   <span class="number">37.85</span> <span class="number">1.09e-14</span> ***</span><br><span class="line">---</span><br><span class="line">Signif. codes:  <span class="number">0</span> ‘***’ <span class="number">0.001</span> ‘**’ <span class="number">0.01</span> ‘*’ <span class="number">0.05</span> ‘.’ <span class="number">0.1</span> ‘ ’ <span class="number">1</span></span><br><span class="line"></span><br><span class="line">Residual standard error: <span class="number">1.525</span> on <span class="number">13</span> degrees of freedom</span><br><span class="line">Multiple R-squared:  <span class="number">0.991</span>,	Adjusted R-squared:  <span class="number">0.9903</span> </span><br><span class="line"><span class="literal">F</span>-statistic:  <span class="number">1433</span> on <span class="number">1</span> and <span class="number">13</span> DF,  p-value: <span class="number">1.091e-14</span></span><br><span class="line"></span><br><span class="line">&gt; plot(women$height,women$weight,</span><br><span class="line">+      xlab=<span class="string">"Height (in inches)"</span>,</span><br><span class="line">+      ylab=<span class="string">"Weight (in pounds)"</span>)</span><br><span class="line">&gt; abline(fit)</span><br></pre></td></tr></table></figure>
<p>从结果可以看出：</p>
<ol type="1">
<li><strong>回归系数(3.45)显著不为0(p&lt;0.001)</strong>，表明身高每增高1英寸，体重将预期增加 3.45磅1；</li>
<li><strong>R平方项</strong>(0.991)表明模型可以解释体重99.1%的方差，它也是实际和预测值之间相关系数的平方(<span class="math inline">\(R^{2}=r^{2}_{\hat{Y} Y}\)</span>)；</li>
<li><strong>残差标准误</strong>(1.53 lbs)则可认为是模型用身高预测体重的平均误差；</li>
<li><strong>F统计量</strong>检验所有的预测变量预测响应变量是否都在某个几率水平之上。由于简单回归只有一个预 测变量，此处F检验等同于身高回归系数的t检验。</li>
</ol>
<p>以下是图示：</p>
<p><img src="https://res.cloudinary.com/vpointer/image/upload/v1587878880/R/简单线性回归.png" style="zoom:50%;" /></p>
<h3 id="多项式回归">多项式回归</h3>
<p>从上图可以看出，或许可以通过添加一个二次项来提高回归的预测精度：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&gt; fit2 &lt;- lm(weight ~ height + I(height^<span class="number">2</span>), data=women)</span><br><span class="line">&gt; summary(fit2)</span><br><span class="line"></span><br><span class="line">Call:</span><br><span class="line">lm(formula = weight ~ height + I(height^<span class="number">2</span>), data = women)</span><br><span class="line"></span><br><span class="line">Residuals:</span><br><span class="line">     Min       1Q   Median       3Q      Max </span><br><span class="line">-<span class="number">0.50941</span> -<span class="number">0.29611</span> -<span class="number">0.00941</span>  <span class="number">0.28615</span>  <span class="number">0.59706</span> </span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">             Estimate Std. Error t value Pr(&gt;|t|)    </span><br><span class="line">(Intercept) <span class="number">261.87818</span>   <span class="number">25.19677</span>  <span class="number">10.393</span> <span class="number">2.36e-07</span> ***</span><br><span class="line">height       -<span class="number">7.34832</span>    <span class="number">0.77769</span>  -<span class="number">9.449</span> <span class="number">6.58e-07</span> ***</span><br><span class="line">I(height^<span class="number">2</span>)   <span class="number">0.08306</span>    <span class="number">0.00598</span>  <span class="number">13.891</span> <span class="number">9.32e-09</span> ***</span><br><span class="line">---</span><br><span class="line">Signif. codes:  <span class="number">0</span> ‘***’ <span class="number">0.001</span> ‘**’ <span class="number">0.01</span> ‘*’ <span class="number">0.05</span> ‘.’ <span class="number">0.1</span> ‘ ’ <span class="number">1</span></span><br><span class="line"></span><br><span class="line">Residual standard error: <span class="number">0.3841</span> on <span class="number">12</span> degrees of freedom</span><br><span class="line">Multiple R-squared:  <span class="number">0.9995</span>,	Adjusted R-squared:  <span class="number">0.9994</span> </span><br><span class="line"><span class="literal">F</span>-statistic: <span class="number">1.139e+04</span> on <span class="number">2</span> and <span class="number">12</span> DF,  p-value: &lt; <span class="number">2.2e-16</span></span><br><span class="line"></span><br><span class="line">&gt; plot(women$height,women$weight,</span><br><span class="line">+      xlab=<span class="string">"Height (in inches)"</span>,</span><br><span class="line">+      ylab=<span class="string">"Weight (in lbs)"</span>)</span><br><span class="line">&gt; lines(women$height,fitted(fit2))</span><br></pre></td></tr></table></figure>
<p><img src="https://res.cloudinary.com/vpointer/image/upload/v1587879332/R/多项式回归.png" style="zoom:50%;" /></p>
<p>模型的方差解释率已经增加到了99.9%。二次项的显著性(t=13.89，p&lt;0.001)表明包含二次项提高了模型的拟合度。</p>
<h3 id="更方便的函数scatterplot">更方便的函数：scatterplot()</h3>
<p>第三方包car中的<code>scatterplot()</code>函数可以更方便的绘制二元关系图：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(car)</span><br><span class="line">scatterplot(weight ~ height, data = women, pch = <span class="number">19</span>, smooth = list(spread = <span class="literal">F</span>), </span><br><span class="line">            main = <span class="string">"Women Age 30-39"</span>, </span><br><span class="line">            xlab = <span class="string">"Height (inches)"</span>, ylab = <span class="string">"Weight (lbs.)"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://res.cloudinary.com/vpointer/image/upload/v1587883551/R/scatterplot.png" style="zoom:50%;" /></p>
<h3 id="有交互项的多元线性回归">有交互项的多元线性回归</h3>
<p><strong>多元回归分析中，第一步最好检查一下变量间的相关性。</strong>可以使用car包中的<code>scatterplotMatrix()</code>函数：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; cars &lt;- as.data.frame(mtcars[, c(<span class="string">"hp"</span>, <span class="string">"wt"</span>, <span class="string">"mpg"</span>)])</span><br><span class="line">&gt; <span class="keyword">library</span>(car)</span><br><span class="line">&gt; scatterplotMatrix(cars, smooth = list(spread = <span class="literal">F</span>), main = <span class="string">"Scatter Plot Matrix"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://res.cloudinary.com/vpointer/image/upload/v1587884604/R/scatterplotMatrix.png" style="zoom:50%;" /></p>
<p><code>scatterplotMatrix()</code>函数默认在非对角线区域绘制变量间的散点图，并添加平滑和线性拟合曲线。对角线区域绘制每个变量的密度图和轴须图。</p>
<p>以下拟合一个带交互项的多元线性回归函数：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&gt; fit &lt;- lm(mpg ~ hp + wt + hp:wt, data=mtcars)</span><br><span class="line">&gt; summary(fit)</span><br><span class="line"></span><br><span class="line">Call:</span><br><span class="line">lm(formula = mpg ~ hp + wt + hp:wt, data = mtcars)</span><br><span class="line"></span><br><span class="line">Residuals:</span><br><span class="line">    Min      1Q  Median      3Q     Max </span><br><span class="line">-<span class="number">3.0632</span> -<span class="number">1.6491</span> -<span class="number">0.7362</span>  <span class="number">1.4211</span>  <span class="number">4.5513</span> </span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">            Estimate Std. Error t value Pr(&gt;|t|)    </span><br><span class="line">(Intercept) <span class="number">49.80842</span>    <span class="number">3.60516</span>  <span class="number">13.816</span> <span class="number">5.01e-14</span> ***</span><br><span class="line">hp          -<span class="number">0.12010</span>    <span class="number">0.02470</span>  -<span class="number">4.863</span> <span class="number">4.04e-05</span> ***</span><br><span class="line">wt          -<span class="number">8.21662</span>    <span class="number">1.26971</span>  -<span class="number">6.471</span> <span class="number">5.20e-07</span> ***</span><br><span class="line">hp:wt        <span class="number">0.02785</span>    <span class="number">0.00742</span>   <span class="number">3.753</span> <span class="number">0.000811</span> ***</span><br><span class="line">---</span><br><span class="line">Signif. codes:  <span class="number">0</span> ‘***’ <span class="number">0.001</span> ‘**’ <span class="number">0.01</span> ‘*’ <span class="number">0.05</span> ‘.’ <span class="number">0.1</span> ‘ ’ <span class="number">1</span></span><br><span class="line"></span><br><span class="line">Residual standard error: <span class="number">2.153</span> on <span class="number">28</span> degrees of freedom</span><br><span class="line">Multiple R-squared:  <span class="number">0.8848</span>,	Adjusted R-squared:  <span class="number">0.8724</span> </span><br><span class="line"><span class="literal">F</span>-statistic: <span class="number">71.66</span> on <span class="number">3</span> and <span class="number">28</span> DF,  p-value: <span class="number">2.981e-13</span></span><br></pre></td></tr></table></figure>
<p>当预测变量不止一个时，如果不带交互项，那么<strong>回归系数的含义很明显：当其他自变量不变时，一个自变量增加一个单位，因变量将要增加的数量。</strong>但是由于此处带了交互项，固定其他自变量时，需要加上交互项。</p>
<p>当交互项显著时，说明因变量与其中一个自变量的关系依赖于另外一个自变量。</p>
<h1 id="回归诊断">回归诊断</h1>
<p>拟合模型只不过是分析的第一步，一旦拟合了回归模型，必须对方法中暗含的统计假设进行检验。</p>
<p>回顾之前的过程，你可能先是做出了散点矩阵来观测各个变量之间的相关性，然后通过<code>lm()</code>函数拟合了一个线性函数，最后再用<code>summary()</code>函数获取了模型参数和相关统计量。</p>
<p>然而，<code>summary()</code>函数并没有评价模型是否合适。<strong>对模型参数推断的信心依赖于它在多大程度上满足OLS模型统计假设</strong>，但是<code>summary()</code>也没有提供任何此方面的信息。</p>
<p>回归诊断技术向你提供了评价回归模型适用性的必要工具，它能帮助发现并纠正问题。</p>
<h2 id="标准方法">标准方法</h2>
<p>最常见的方法就是对<code>lm()</code>函数返回的对象使用<code>plot()</code>函数，可以生成评价模型拟合情况的四幅图形：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fit &lt;- lm(weight ~ height, data = women)</span><br><span class="line">par(mfrow = c(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">plot(fit)</span><br></pre></td></tr></table></figure>
<p><img src="https://res.cloudinary.com/vpointer/image/upload/v1587885966/R/lmplot.png" /></p>
<p>为了解释上面的4张图，需要回顾OSL回归的统计假设：</p>
<ul>
<li>正态性：<strong>当自变量的值固定时，因变量成正态分布，则残差值也应该是一个均值为0的正态分布</strong>。“正态Q-Q图”(Normal Q-Q，右上)是在正态分布对应的值下，标准化残差的概率图。若满足正态假设，那么图上的点应该落在呈45度角的直线上；</li>
<li>独立性：上述4图无法体现出因变量的值是否相互独立，可以根据收集数据的方式来验证；</li>
<li>线性：<strong>若因变量与自变量线性相关，那么残差值与预测(拟合)值就没有任何系统关联</strong>。换句话说，除了白噪声，模型应该包含数据中所有的系统方差。在“残差图与拟合图” (Residuals vs Fitted，左上)中可以清楚地看到一个曲线关系，这暗示着你可能需要对回归模型加上一个二次项；</li>
<li>同方差性：若满足不变方差假设，那么<strong>在“位置尺度图”中(Scale-Location Graph，左下)，水平线周围的点应该随机分布</strong>。</li>
</ul>
<p>总结： - 线性：残差图与拟合图 Residuals vs Fitted，曲线附近的点应该是随机的 - 正态性：正态Q-Q图 Normal Q-Q，所有点应该分布在45度角的斜线上 - 同方差性：位置尺度图 Scale-Location，曲线附近的点应该是随机的</p>
<p>最后一幅“残差与杠杆图”(Residuals vs Leverage，右下)提供了你可能关注的单个观测点的信息。从图形可以鉴别出离群点、高杠杆值点和强影响点：</p>
<ul>
<li>离群点：表明拟合回归模型对其预测效果不佳(产生了巨大的或正或负的残差)；</li>
<li>一个观测点有很高的杠杆值：表明它是一个异常的预测变量值的组合。也就是说，在预测变量空间中，它是一个离群点。因变量值不参与计算一个观测点的杠杆值；</li>
<li>一个观测点是强影响点(influential observation)：表明它对模型参数的估计产生的影响过大，非常不成比例。强影响点可以通过Cook距离即Cook’s D统计量来鉴别。<strong>强影响点可以删除，但需要谨慎</strong>。</li>
</ul>
<p>至于怎么看这个残差与杠杆图，笔者也不清楚。。。而且它的可读性太差且不够实用，后面会有更好的方法，所以不了解也不碍事。</p>
<h2 id="改进的方法">改进的方法</h2>
<p>主要介绍的是car包中的以下回归诊断函数以及gvlma包：</p>
<table>
<thead>
<tr class="header">
<th>函数</th>
<th>目的</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>qqPlot()</code></td>
<td>分位数比较图</td>
</tr>
<tr class="even">
<td><code>durbinWatsonTest()</code></td>
<td>对误差自相关性做 Durbin-Watson 检验</td>
</tr>
<tr class="odd">
<td><code>crPlots()</code></td>
<td>成分与残差图</td>
</tr>
<tr class="even">
<td><code>ncvTest()</code></td>
<td>对非恒定的误差方差做得分检验</td>
</tr>
<tr class="odd">
<td><code>spreadLevelPlot()</code></td>
<td>分散水平检验</td>
</tr>
<tr class="even">
<td><code>outlierTest()</code></td>
<td>Bonferroni 离群点检验</td>
</tr>
<tr class="odd">
<td><code>avPlots()</code></td>
<td>添加的变量图形</td>
</tr>
<tr class="even">
<td><code>inluencePlot()</code></td>
<td>回归影响图</td>
</tr>
<tr class="odd">
<td><code>vif()</code></td>
<td>方差膨胀因子</td>
</tr>
</tbody>
</table>
<h3 id="检验正态性">检验正态性</h3>
<p><code>qqPlot()</code>函数提供了更为精确的正态假设检验方法，它画出了在n–p–1个自由度的t分布下的学生化残差图形(studentized residual，也称<strong>学生化删除残差</strong>或<strong>折叠化残差</strong>)，其中<code>n</code>是样本大小，<code>p</code>是回归参数的数目(包括截距项)：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(car)</span><br><span class="line">states &lt;- as.data.frame(state.x77[, c(<span class="string">"Murder"</span>, <span class="string">"Population"</span>, <span class="string">"Illiteracy"</span>, <span class="string">"Income"</span>, <span class="string">"Frost"</span>)])</span><br><span class="line">fit &lt;- lm(Murder ~ Population + Illiteracy + Income + Frost, data = states)</span><br><span class="line">qqPlot(fit, labels = row.names(states), simulate = <span class="literal">TRUE</span>, main = <span class="string">"Q-Q Plot"</span>)</span><br></pre></td></tr></table></figure>
<p>当<code>simulate=TRUE</code>时，95%的置信区间将会用<strong>参数自助法</strong>生成，它会使得每次运行所得到的结果都不相同。以下的两条虚线就表示置信区间。</p>
<p><img src="https://res.cloudinary.com/vpointer/image/upload/v1587889862/R/qqplotcar.png" style="zoom:50%;" /></p>
<p>此处可以看出，Nevada这个点存在异常。除了Nevada，所有的点都离直线很近，并都落在置信区间内，这表明正态性假设符合得很好。 但是你也必须关注Nevada，它有一个很大的正残差值(<code>真实值-预测值</code>)，表明模型低估了该州的谋杀率。</p>
<h3 id="检验误差的独立性">检验误差的独立性</h3>
<p>之前提过，判断因变量值(或残差)是否相互独立，最好的方法是依据收集数据方式。不过car包提供了一个可做Durbin-Watson检验的函数，能够检测误差的序列相关性：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; durbinWatsonTest(fit)</span><br><span class="line"> lag Autocorrelation D-W Statistic p-value</span><br><span class="line">   <span class="number">1</span>      -<span class="number">0.2006929</span>      <span class="number">2.317691</span>    <span class="number">0.25</span></span><br><span class="line"> Alternative hypothesis: rho != <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>p值不显著(p=0.282)说明无自相关性，误差项之间独立。</p>
<p>滞后项(lag=1)表明数据集中每个数据都是与其后一个数据进行比较的。</p>
<p><strong>该检验适用于时间独立的数据，对于非聚集型的数据并不适用</strong>。</p>
<h3 id="检验线性">检验线性</h3>
<p>可以通过car包中的<code>crPlots()</code>函数绘制成分残差图(component plus residual plot，也称偏残差(partial residual plot)来查看自变量与因变量之间是否有线性关系，也可以通过它查看是否有不同于已设定线性模型的系统偏差：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">library</span>(car)</span><br><span class="line">&gt; summary(fit)</span><br><span class="line">Call:</span><br><span class="line">lm(formula = Murder ~ Population + Illiteracy + Income + Frost, </span><br><span class="line">    data = states)</span><br><span class="line"></span><br><span class="line">Residuals:</span><br><span class="line">    Min      1Q  Median      3Q     Max </span><br><span class="line">-<span class="number">4.7960</span> -<span class="number">1.6495</span> -<span class="number">0.0811</span>  <span class="number">1.4815</span>  <span class="number">7.6210</span> </span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">             Estimate Std. Error t value Pr(&gt;|t|)    </span><br><span class="line">(Intercept) <span class="number">1.235e+00</span>  <span class="number">3.866e+00</span>   <span class="number">0.319</span>   <span class="number">0.7510</span>    </span><br><span class="line">Population  <span class="number">2.237e-04</span>  <span class="number">9.052e-05</span>   <span class="number">2.471</span>   <span class="number">0.0173</span> *  </span><br><span class="line">Illiteracy  <span class="number">4.143e+00</span>  <span class="number">8.744e-01</span>   <span class="number">4.738</span> <span class="number">2.19e-05</span> ***</span><br><span class="line">Income      <span class="number">6.442e-05</span>  <span class="number">6.837e-04</span>   <span class="number">0.094</span>   <span class="number">0.9253</span>    </span><br><span class="line">Frost       <span class="number">5.813e-04</span>  <span class="number">1.005e-02</span>   <span class="number">0.058</span>   <span class="number">0.9541</span>    </span><br><span class="line">---</span><br><span class="line">Signif. codes:  <span class="number">0</span> ‘***’ <span class="number">0.001</span> ‘**’ <span class="number">0.01</span> ‘*’ <span class="number">0.05</span> ‘.’ <span class="number">0.1</span> ‘ ’ <span class="number">1</span></span><br><span class="line"></span><br><span class="line">Residual standard error: <span class="number">2.535</span> on <span class="number">45</span> degrees of freedom</span><br><span class="line">Multiple R-squared:  <span class="number">0.567</span>,	Adjusted R-squared:  <span class="number">0.5285</span> </span><br><span class="line"><span class="literal">F</span>-statistic: <span class="number">14.73</span> on <span class="number">4</span> and <span class="number">45</span> DF,  p-value: <span class="number">9.133e-08</span></span><br><span class="line"></span><br><span class="line">&gt; crPlots(fit)</span><br></pre></td></tr></table></figure>
<p><img src="https://res.cloudinary.com/vpointer/image/upload/v1587893311/R/crplot.png" /></p>
<p><strong>通过实线与虚线是否一致来判断线性关系</strong>。上图结果和<code>summary()</code>的结果一致。</p>
<h3 id="检验同方差性">检验同方差性</h3>
<p>car包中有两个函数可以用来判断方差是否恒定。</p>
<h4 id="ncvtest">ncvTest()</h4>
<p><code>ncvTest()</code>函数生成一个计分检验，<strong>零假设为误差方差不变，备择假设为误差方差随着拟合值水平的变化而变化</strong>。若检验显著， 则说明存在异方差性(误差方差不恒定)。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; ncvTest(fit)</span><br><span class="line">Non-constant Variance Score Test </span><br><span class="line">Variance formula: ~ fitted.values </span><br><span class="line">Chisquare = <span class="number">1.746514</span>, Df = <span class="number">1</span>, p = <span class="number">0.18632</span></span><br></pre></td></tr></table></figure>
<p>结果不显著，接受原假设，方差不变。</p>
<h4 id="spreadlevelplot">spreadLevelPlot()</h4>
<p><span id="spreadLevelPlot"><code>spreadLevelPlot()</code></span>创建一个添加了最佳拟合曲线的散点图，展示标准化残差绝对值与拟合值的关系。如果图中的点在拟合的直线周围随机分布，则接受原假设（方差恒定）：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; spreadLevelPlot(fit)</span><br><span class="line"></span><br><span class="line">Suggested power transformation:  <span class="number">1.209626</span></span><br></pre></td></tr></table></figure>
<p><img src="https://res.cloudinary.com/vpointer/image/upload/v1587895945/R/spreadLevelPlot.png" style="zoom:50%;" /></p>
<p>结果除了一张图之外，还输出了一个“建议幂次变换”(suggested power transformation)，它的含义是：经过p次幂(<span class="math inline">\(Y^{p}\)</span>)变换，非恒定的误差方差将会平稳。注意，<strong>此处是对因变量的变换</strong>！如果<strong>建议幂次为0，则做对数变换</strong>。</p>
<h2 id="线性模型假设的综合验证">线性模型假设的综合验证</h2>
<p>上一节中，对OSL的4个假设的检验是分开做的，其实还可以通过gvlma包中的<code>gvlma()</code>函数进行综合检验，该函数同时还能做偏斜度、峰度和异方差性的评价：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">library</span>(gvlma)</span><br><span class="line">&gt; gvmodel &lt;- gvlma(fit)</span><br><span class="line">&gt; summary(gvmodel)	<span class="comment"># 它还会输出summary(fit)的内容</span></span><br><span class="line"><span class="keyword">...</span> <span class="comment"># 省略了summary(fit)的输出内容</span></span><br><span class="line">ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS</span><br><span class="line">USING THE GLOBAL TEST ON <span class="number">4</span> DEGREES-OF-FREEDOM:</span><br><span class="line">Level of Significance =  <span class="number">0.05</span> </span><br><span class="line"></span><br><span class="line">Call:</span><br><span class="line"> gvlma(x = fit) </span><br><span class="line"></span><br><span class="line">                    Value p-value                Decision</span><br><span class="line">Global Stat        <span class="number">2.7728</span>  <span class="number">0.5965</span> Assumptions acceptable.</span><br><span class="line">Skewness           <span class="number">1.5374</span>  <span class="number">0.2150</span> Assumptions acceptable.</span><br><span class="line">Kurtosis           <span class="number">0.6376</span>  <span class="number">0.4246</span> Assumptions acceptable.</span><br><span class="line">Link Function      <span class="number">0.1154</span>  <span class="number">0.7341</span> Assumptions acceptable.</span><br><span class="line">Heteroscedasticity <span class="number">0.4824</span>  <span class="number">0.4873</span> Assumptions acceptable.</span><br></pre></td></tr></table></figure>
<p>Global Stat显示数据满足OLS回归模型所有的统计假设 (p=0.597)。如果拒绝原假设，可以再用之前介绍的方法查找到底是哪一项不成立。</p>
<h2 id="多重共线性">多重共线性</h2>
<p>多重共线性简单说就是自变量之间具有相关性，例如出生日期和年龄做自变量的话，这两个自变量显然是相关的。</p>
<p>多重共线性会导致模型参数的置信区间过大，使单个系数解释起来很困难。</p>
<p>VIF的平方根表示变量回归参数的置信区间能膨胀为与模型无关的预测变量的程度。</p>
<p><strong>一般原则下，<code>vif &gt; 4</code>就表明存在多重共线性问题，<code>vif &gt; 10</code>表示存在严重的多重共线性</strong>。</p>
<p>如果仅仅是做预测，那么多重共线性并不构成问题，但是如果还要对每个预测变量进行解释，那么就必须解决这个问题。</p>
<p>计算自变量的VIF值可以使用car包中的<code>vif()</code>函数：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; vif(fit)</span><br><span class="line">Population Illiteracy     Income      Frost </span><br><span class="line">  <span class="number">1.245282</span>   <span class="number">2.165848</span>   <span class="number">1.345822</span>   <span class="number">2.082547</span></span><br></pre></td></tr></table></figure>
<p>均没有大于4，说明这几个自变量之间没有多重共线性。</p>
<h1 id="异常值分析">异常值分析</h1>
<p>一个全面的回归分析要覆盖对异常值的分析，包括离群点、高杠杆值点和强影响点，因为它们在一定程度上与其他观测点不同，可能对结果产生较大的负面影响。</p>
<h2 id="离群点">离群点</h2>
<p>离群点是指那些模型预测效果不佳的观测点。它们通常有很大的、或正或负的残差(<span class="math inline">\(Y_{i}-\hat{Y}_{i}\)</span>)。 正的残差说明模型低估了响应值，负的残差则说明高估了响应值。</p>
<p>之前的内容已经展示了<strong>一种检测离群点的方法：正态Q-Q图</strong>。</p>
<p>另外<strong>一个粗糙的判断准则：标准化残差值大于2或者小于–2的点可能是离群点</strong>。</p>
<p>car包也提供了一种离群点的统计检验方法。<code>outlierTest()</code>函数可以求得最大标准化残差绝对值Bonferroni调整后的p值:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fit依然是上一节中根据states拟合的模型</span></span><br><span class="line">&gt; outlierTest(fit)</span><br><span class="line">       rstudent unadjusted p-value Bonferroni p</span><br><span class="line">Nevada <span class="number">3.542929</span>         <span class="number">0.00095088</span>     <span class="number">0.047544</span></span><br></pre></td></tr></table></figure>
<p>注意，该函数只是根据单个最大(或正或负)残差值的显著性来判断是否有离群点。<strong>若不显著，则说明数据集中没有离群点;若显著，必须删除该离群点，然后再检验是否还有其他离群点（相当于一次只判断一个，直到没有）。</strong></p>
<h2 id="高杠杆点">高杠杆点</h2>
<p>高杠杆值观测点，即<strong>与其他预测变量有关的离群点</strong>。换句话说，它们是<strong>由许多异常的预测变量值组合起来的，与响应变量值没有关系</strong>。</p>
<p>高杠杆值的观测点可通过帽子统计量(hat statistic)判断。对于一个给定的数据集，帽子均值为<code>p/n</code>，其中<code>p</code>是模型估计的参数数目(包含截距项)，<code>n</code>是样本量。一般来说，若观测点的帽子值大于帽子均值的2或3倍，就可以认定为高杠杆值点。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hat.plot &lt;- <span class="keyword">function</span>(fit) &#123;</span><br><span class="line">    p &lt;- length(coefficients(fit))</span><br><span class="line">    n &lt;- length(fitted(fit))</span><br><span class="line">    hts &lt;- hatvalues(fit)</span><br><span class="line">    </span><br><span class="line">    idx &lt;- hts &gt; (<span class="number">2</span> * p/n)</span><br><span class="line">    </span><br><span class="line">    plot(hts, main = <span class="string">"Index Plot of Hat Values"</span>)</span><br><span class="line">    abline(h = c(<span class="number">2</span>, <span class="number">3</span>) * p/n, col = <span class="string">"red"</span>, lty = <span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    text(c(<span class="number">1</span>:n)[idx], hts[idx], names(hts)[idx], pos = <span class="number">1</span>)</span><br><span class="line">&#125;</span><br><span class="line">hat.plot(fit)</span><br></pre></td></tr></table></figure>
<p><img src="https://res.cloudinary.com/vpointer/image/upload/v1587913038/R/hatvalues.png" style="zoom:50%;" /></p>
<p>从上图可以看出，Alaska和California这两个点的杠杆值非常高。</p>
<p><strong>高杠杆值点可能是强影响点，也可能不是，这要看它们是否是离群点。</strong></p>
<h2 id="强影响点">强影响点</h2>
<p>强影响点，即<strong>对模型参数估计值影响有些比例失衡的点</strong>。例如，若移除模型的一个观测点时模型会发生巨大的改变，这种点就是强影响点。</p>
<p>有两种方法可以检测强影响点：Cook距离，又称D统计量以及变量添加图(added variable plot)。</p>
<h3 id="cook距离">Cook距离</h3>
<p>一般来说，Cook’s D值大于<code>4/(n–k–1)</code>，则表明它是强影响点，其中<code>n</code>为样本量大小，<code>k</code>是预测变量数目。</p>
<p>可以通过以下代码作图判断强影响点：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cutoff &lt;- <span class="number">4</span>/(nrow(states)-length(fit$coefficients)-<span class="number">2</span>)</span><br><span class="line">plot(fit, which=<span class="number">4</span>, cook.levels=cutoff)</span><br><span class="line">abline(h=cutoff, lty=<span class="number">2</span>, col=<span class="string">"red"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://res.cloudinary.com/vpointer/image/upload/v1587914726/R/cookd.png" style="zoom:50%;" /></p>
<p>可以看到Alaska、Hawaii和Nevada是强影响点。</p>
<h3 id="变量添加图">变量添加图</h3>
<p>Cook’s D图有助于鉴别强影响点，但是并不提供关于这些点如何影响模型的信息。<strong>变量添加图</strong>弥补了这个缺陷，可以通过car包中的<code>avPlots()</code>函数绘制：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">avPlots(fit)</span><br></pre></td></tr></table></figure>
<p><img src="https://res.cloudinary.com/vpointer/image/upload/v1587914985/R/avplots.png" /></p>
<p><strong>图中的直线表示相应预测变量的实际回归系数</strong>。上述诸图就是方便大家想象删除某些强影响点后直线的变化，以此来估计它的影响效果。例如，对于左下角这幅图，如果删除Alaska和Nevada这两个，直线将变为负斜率。</p>
<h2 id="综合分析异常值">综合分析异常值</h2>
<p>car包中的<code>influencePlot()</code>函数可以将离群点、杠杆值和强影响点的信息整合到一幅图形中：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; influencePlot(fit, main = <span class="string">"Influence Plot"</span>, sub = <span class="string">"Circle size is proportional to Cook's distance"</span>)</span><br><span class="line">                StudRes        Hat       CookD</span><br><span class="line">Alaska        <span class="number">1.7536917</span> <span class="number">0.43247319</span> <span class="number">0.448050997</span></span><br><span class="line">California   -<span class="number">0.2761492</span> <span class="number">0.34087628</span> <span class="number">0.008052956</span></span><br><span class="line">Nevada        <span class="number">3.5429286</span> <span class="number">0.09508977</span> <span class="number">0.209915743</span></span><br><span class="line">Rhode Island -<span class="number">2.0001631</span> <span class="number">0.04562377</span> <span class="number">0.035858963</span></span><br></pre></td></tr></table></figure>
<p>同时还会生成一张<strong>影响图</strong>：</p>
<p><img src="https://res.cloudinary.com/vpointer/image/upload/v1587915753/R/influence.png" style="zoom:50%;" /></p>
<p>说明：</p>
<ul>
<li>纵坐标超过+2或小于–2的州可被认为是离群点；</li>
<li>水平轴超过0.2或0.3的州有高杠杆值(通常为预测值的组合)；</li>
<li>圆圈大小与影响成比例，圆圈很大的点可能是对模型参数的估计造成的不成比例影响的强影响点。</li>
</ul>
<p>可以得出如下结论：</p>
<ul>
<li>离群点：Nevada和Rhode Island</li>
<li>高杠杆点：New York、California、Hawaii和Washington</li>
<li>强影响点：Nevada、Alaska和Hawaii</li>
</ul>
<h1 id="改进模型">改进模型</h1>
<p>以上已经估计出了模型，然后也对模型进行了诊断，随后还分析了异常点，那么下一步就是根据前面的分析改进模型了。</p>
<p>四种方式可以处理违背回归假设的问题：</p>
<ol type="1">
<li>删除观测点；</li>
<li>变量变换；</li>
<li>添加或删除变量；</li>
<li>使用其他回归方法。</li>
</ol>
<h2 id="删除观测点">删除观测点</h2>
<p>删除离群点通常可以提高数据集对于正态假设的拟合度；强影响点会干扰结果，通常也会被删除。</p>
<p>删除最大的离群点或者强影响点后，模型需要重新拟合。</p>
<p>若离群点或强影响点仍然存在， 重复以上过程直至获得比较满意的拟合。</p>
<p>但<strong>删除观测点需谨慎</strong>：如果是人为因素造成的，比如记错了，那么删除是合理的；但如果这就是个正确的数据，那么删除它们可能就不太合适了，发掘为何该观测点不同于其他点，有助于更深刻地理解研究的主题以及发现其他可能没有想过的问题。</p>
<h2 id="变量变换">变量变换</h2>
<p><strong>说在最前</strong>：与“删除观测点”相同的是，<strong>请谨慎使用变量变换</strong>！首先，你需要解释这样变换的意义是什么；其次，如果变换了变量，你的解释必须基于变换后的变量，而不是初始变量。如果无法解释，请谨慎变换！</p>
<p>变换多用<span class="math inline">\(Y^{\lambda}\)</span>替代<span class="math inline">\(Y\)</span>： <img src="https://res.cloudinary.com/vpointer/image/upload/v1587959596/R/y变换.png" /></p>
<p>若<span class="math inline">\(Y\)</span>是比例数，通常使用logit变换<code>ln (Y/1–Y)</code>。</p>
<h3 id="模型违反正态假设时">模型违反正态假设时</h3>
<p>通常可以对因变量尝试某种变换。car包中的<code>powerTransform()</code>函数通过λ的最大似然估计来正态化变量<span class="math inline">\(Y^{\lambda}\)</span>：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt; summary(powerTransform(states$Murder))</span><br><span class="line">bcPower Transformation to Normality </span><br><span class="line">              Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd</span><br><span class="line">states$Murder    <span class="number">0.6055</span>           <span class="number">1</span>       <span class="number">0.0884</span>       <span class="number">1.1227</span></span><br><span class="line"></span><br><span class="line">Likelihood ratio test that transformation parameter is equal to <span class="number">0</span></span><br><span class="line"> (log transformation)</span><br><span class="line">                           LRT df     pval</span><br><span class="line">LR test, lambda = (<span class="number">0</span>) <span class="number">5.665991</span>  <span class="number">1</span> <span class="number">0.017297</span></span><br><span class="line"></span><br><span class="line">Likelihood ratio test that no transformation is needed</span><br><span class="line">                           LRT df    pval</span><br><span class="line">LR test, lambda = (<span class="number">1</span>) <span class="number">2.122763</span>  <span class="number">1</span> <span class="number">0.14512</span></span><br></pre></td></tr></table></figure>
<p>结果表明，你可以用Murder<span class="math inline">\(^{0.6}\)</span>来正态化变量Murder。但在本例中，<code>λ=1</code>的假设也无法拒绝(p=0.145)，因此没 有强有力的证据表明本例需要变量变换。</p>
<h3 id="模型违反线性假设时">模型违反线性假设时</h3>
<p>此时对预测变量进行变换会比较有用。car包中的<code>boxTidwell()</code>函数通过获得预测变量幂数的最大似然估计来改善线性关系。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; boxTidwell(Murder~Population+Illiteracy,data=states)</span><br><span class="line">           MLE of lambda Score Statistic (z) Pr(&gt;|z|)</span><br><span class="line">Population       <span class="number">0.86939</span>             -<span class="number">0.3228</span>   <span class="number">0.7468</span></span><br><span class="line">Illiteracy       <span class="number">1.35812</span>              <span class="number">0.6194</span>   <span class="number">0.5357</span></span><br><span class="line"></span><br><span class="line">iterations =  <span class="number">19</span></span><br></pre></td></tr></table></figure>
<p>结果显示，使用变换<span class="math inline">\(Population^{0.87}\)</span>和<span class="math inline">\(Illiteracy^{1.36}\)</span>能够大大改善线性关系。但是对Population(p=0.75)和Illiteracy(p=0.54)的计分检验又表明变量并不需要变换。</p>
<h3 id="模型违反方差同性假设时">模型违反方差同性假设时</h3>
<p>可以使用<a href="#spreadLevelPlot">之前提到</a>过的car包中的<code>spreadLevelPlot()</code>函数来改善，这里不再赘述，但需要提醒的是，它的建议值是对因变量的变化。</p>
<h2 id="增删变量">增删变量</h2>
<p>有时，添加一个重要变量可以解决之前已经讨论过的许多问题，删除一个冗余变量也能达到同样的效果。</p>
<p>删除变量在处理多重共线性时是一种非常重要的方法。最常见的方法就是删除某个存在多重共线性的变量。</p>
<p>另外一个可用的方法便是岭回归——多元回归的变体，专门用来处理多重共线性问题。</p>
<h1 id="选择最佳回归模型">选择“最佳”回归模型</h1>
<p>本节讨论的问题，就是如何在候选模型中进行筛选。</p>
<h2 id="r自带的模型比较方法">R自带的模型比较方法</h2>
<p>如果只有两个模型，可以使用R自带的方法，多于两个的模型需要使用后续方法。</p>
<p>这里主要介绍的是<code>anova()</code>函数和<code>AIC()</code>函数。</p>
<h3 id="anova比较模型">anova()比较模型</h3>
<p><strong><code>anova()</code>函数可以比较两个嵌套模型的拟合优度。</strong></p>
<p>所谓的嵌套模型是指：模型A的一些项完全包含在模型B中，那么模型A就相当于“嵌套”在模型B中。</p>
<p>依然以前方<code>states</code>的多元回归模型为例，探究<code>Income</code>和<code>Frost</code>这两个自变量是否是必须的：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt; states &lt;- as.data.frame(state.x77[,c(<span class="string">"Murder"</span>, <span class="string">"Population"</span>, <span class="string">"Illiteracy"</span>, <span class="string">"Income"</span>, <span class="string">"Frost"</span>)])</span><br><span class="line">&gt; fit1 &lt;- lm(Murder ~ Population + Illiteracy, data=states)</span><br><span class="line">&gt; fit2 &lt;- lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)</span><br><span class="line">&gt; anova(fit1, fit2)</span><br><span class="line">Analysis of Variance Table</span><br><span class="line"></span><br><span class="line">Model <span class="number">1</span>: Murder ~ Population + Illiteracy</span><br><span class="line">Model <span class="number">2</span>: Murder ~ Population + Illiteracy + Income + Frost</span><br><span class="line">  Res.Df    RSS Df Sum of Sq      <span class="literal">F</span> Pr(&gt;<span class="literal">F</span>)</span><br><span class="line"><span class="number">1</span>     <span class="number">47</span> <span class="number">289.25</span>                           </span><br><span class="line"><span class="number">2</span>     <span class="number">45</span> <span class="number">289.17</span>  <span class="number">2</span>  <span class="number">0.078505</span> <span class="number">0.0061</span> <span class="number">0.9939</span></span><br></pre></td></tr></table></figure>
<p>对于模型2的检验并不显著，说明可以删掉这两个变量。</p>
<h3 id="aic函数比较模型">AIC()函数比较模型</h3>
<p>AIC(Akaike Information Criterion，赤池信息准则)也可以用来比较模型，它考虑了模型的<strong>统计拟合度</strong>以及<strong>用来拟合的参数数目</strong>。<strong>AIC值较小的模型要优先选择，它说明模型用较少的参数获得了足够的拟合度</strong>。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; AIC(fit1, fit2)</span><br><span class="line">     df      AIC</span><br><span class="line">fit1  <span class="number">4</span> <span class="number">237.6565</span></span><br><span class="line">fit2  <span class="number">6</span> <span class="number">241.6429</span></span><br></pre></td></tr></table></figure>
<p><code>AIC()</code>给出的结果依然是应该选择模型1。</p>
<p>注意：<strong>ANOVA需要嵌套模型，AIC不需要</strong>。</p>
<h2 id="模型的自变量选择">模型的自变量选择</h2>
<p>目前有两种流行的方法：<strong>逐步回归法(stepwise method)</strong>和<strong>全子集回归(all-subsets regression)</strong>。</p>
<h3 id="逐步回归">逐步回归</h3>
<p>逐步回归中，模型会一次添加或者删除一个变量，直到达到某个判停准则为止。它又分为3个类型：</p>
<ol type="1">
<li><strong>向前逐步回归</strong>(forward stepwise regression)每次添加一个预测变量到模型中，直到添加变量不会使模型有所改进为止；</li>
<li><strong>向后逐步回归</strong>(backward stepwise regression)从模型包含所有预测变量开始， 一次删除一个变量直到会降低模型质量为止；</li>
<li>向前向后逐步回归(stepwise stepwise regression， 通常称作<strong>逐步回归</strong>，以避免听起来太冗长)，结合了向前逐步回归和向后逐步回归的方法，变量每次进入一个，但是每一步中，变量都会被重新评价，对模型没有贡献的变量将会被删除，预测变量可能会被添加、删除好几次，直到获得最优模型为止。</li>
</ol>
<p>逐步回归算法依据判断准则的不同而不同，这里使用MASS包中的<code>stepAIC()</code>函数，它实现了上述三种算法，依据的是精确AIC准则。</p>
<p>仍以前方的<code>states</code>为例，这里采用的是向后回归：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">library</span>(MASS)</span><br><span class="line">&gt; fit &lt;- lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)</span><br><span class="line">&gt; stepAIC(fit, direction=<span class="string">"backward"</span>)</span><br><span class="line">Start:  AIC=<span class="number">97.75</span></span><br><span class="line">Murder ~ Population + Illiteracy + Income + Frost</span><br><span class="line"></span><br><span class="line">             Df Sum of Sq    RSS     AIC</span><br><span class="line">- Frost       <span class="number">1</span>     <span class="number">0.021</span> <span class="number">289.19</span>  <span class="number">95.753</span></span><br><span class="line">- Income      <span class="number">1</span>     <span class="number">0.057</span> <span class="number">289.22</span>  <span class="number">95.759</span></span><br><span class="line">&lt;none&gt;                    <span class="number">289.17</span>  <span class="number">97.749</span></span><br><span class="line">- Population  <span class="number">1</span>    <span class="number">39.238</span> <span class="number">328.41</span> <span class="number">102.111</span></span><br><span class="line">- Illiteracy  <span class="number">1</span>   <span class="number">144.264</span> <span class="number">433.43</span> <span class="number">115.986</span></span><br><span class="line"></span><br><span class="line">Step:  AIC=<span class="number">95.75</span></span><br><span class="line">Murder ~ Population + Illiteracy + Income</span><br><span class="line"></span><br><span class="line">             Df Sum of Sq    RSS     AIC</span><br><span class="line">- Income      <span class="number">1</span>     <span class="number">0.057</span> <span class="number">289.25</span>  <span class="number">93.763</span></span><br><span class="line">&lt;none&gt;                    <span class="number">289.19</span>  <span class="number">95.753</span></span><br><span class="line">- Population  <span class="number">1</span>    <span class="number">43.658</span> <span class="number">332.85</span> <span class="number">100.783</span></span><br><span class="line">- Illiteracy  <span class="number">1</span>   <span class="number">236.196</span> <span class="number">525.38</span> <span class="number">123.605</span></span><br><span class="line"></span><br><span class="line">Step:  AIC=<span class="number">93.76</span></span><br><span class="line">Murder ~ Population + Illiteracy</span><br><span class="line"></span><br><span class="line">             Df Sum of Sq    RSS     AIC</span><br><span class="line">&lt;none&gt;                    <span class="number">289.25</span>  <span class="number">93.763</span></span><br><span class="line">- Population  <span class="number">1</span>    <span class="number">48.517</span> <span class="number">337.76</span>  <span class="number">99.516</span></span><br><span class="line">- Illiteracy  <span class="number">1</span>   <span class="number">299.646</span> <span class="number">588.89</span> <span class="number">127.311</span></span><br><span class="line"></span><br><span class="line">Call:</span><br><span class="line">lm(formula = Murder ~ Population + Illiteracy, data = states)</span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">(Intercept)   Population   Illiteracy  </span><br><span class="line">  <span class="number">1.6515497</span>    <span class="number">0.0002242</span>    <span class="number">4.0807366</span></span><br></pre></td></tr></table></figure>
<p>上述每一步中的表格表示，删除该变量后，模型最后的AIC值，其中的<code>&lt;none&gt;</code>表示此步没有删除变量时，模型的AIC值。模型每次都按AIC最小值来删除，直到删除变量不能再减小模型AIC值为止。</p>
<h3 id="全子集回归">全子集回归</h3>
<p>逐步回归法其实存在争议，虽然它可能会找到一个好的模型，但是不能保证模型就是最佳模型，因为不是每一个可能的模型都被评价了。</p>
<p><strong>全子集回归会检验所有可能的模型。</strong></p>
<p>全子集回归可用leaps包中的<code>regsubsets()</code>函数实现。可以通过R平方、调整R平方或Mallows Cp统计量等准则来选择“最佳”模型。</p>
<p><strong>R平方是预测变量解释响应变量的程度；调整R平方与之类似，但考虑了模型的参数数目。</strong></p>
<p>R平方总会随着变量数目的增加而增加。当与样本量相比，预测变量数目很大时，容易导致<strong>过拟合</strong>。R平方很可能会丢失数据的偶然变异信息，而<strong>调整R平方则提供了更为真实的R平方估计，也就是说，对于调整R平方，它的值越大越好</strong>。</p>
<p>另外，Mallows Cp统计量也用来作为逐步回归的判停规则。广泛研究表明，<strong>对于一个好的模型，它的Cp统计量非常接近于模型的参数数目(包括截距项)</strong>。</p>
<p>以下是一个使用调整R平方的例子：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(leaps)</span><br><span class="line">leaps &lt;- regsubsets(Murder ~ Population + Illiteracy + Income + Frost, data=states, nbest=<span class="number">4</span>)</span><br><span class="line">plot(leaps, scale=<span class="string">"adjr2"</span>) <span class="comment"># 是leaps包中的函数，而不是自带的plot函数</span></span><br></pre></td></tr></table></figure>
<p><img src="https://res.cloudinary.com/vpointer/image/upload/v1587966028/R/regsubsets.png" style="zoom:50%;" /></p>
<p>参数<code>nbest</code>表示展示n个不同子集大小(一个、两个或多个预测变量)中的最佳模型，例如，<code>nbest=4</code>表示先展示4个最佳的单预测变量模型，然后展示4个最佳的双预测变量模型，以此类推，直到包含所有的预测变量。</p>
<p>最上面一行展示的就是“最佳”模型，即只含Population和Illiteracy这两个自变量的模型。</p>
<h1 id="深层次分析">深层次分析</h1>
<p>本节介绍如何评价<strong>模型泛化能力</strong>和<strong>变量相对重要性</strong>。</p>
<p>如果只是拟合一个方程来进行描述性分析，而不涉及现新数据的预测，那么到上一节就可以了。</p>
<p>本节涉及的是用拟合的模型进行预测的问题。</p>
<h2 id="交叉验证">交叉验证</h2>
<p>对于OLS回归，通过让预测误差(残差)平方和最小和对响应变量的解释度(R平方)最大，可获得模型参数。但这只是在已给定的数据集上“最佳”，在新数据集上表现并不一定好，还需要进一步验证。</p>
<p><strong>交叉验证法可以评价回归方程的泛化能力</strong>。</p>
<p>所谓交叉验证，即将一定比例的数据挑选出来作为<strong>训练样本</strong>，另外的样本作<strong>保留样本</strong>，先在训练样本上获取回归方程，然后在保留样本上做预测。由于保留样本不涉及模型参数的选择，该样本可获得比新数据更为精确的估计。</p>
<p>在<strong>k重交叉验证</strong>中，样本被分为k个子样本，轮流将k–1个子样本组合作为训练集，另外1个子样本作为保留集。这样会获得k个预测方程，记录k个保留样本的预测表现结果，然后求其平均值。(当n是观测总数目，且k为n时，该方法又称作刀切法，jackknifing)</p>
<p><strong>bootstrap包中的<code>crossval()</code>函数可以实现k重交叉验证。</strong></p>
<p>以下自定义的<code>shrinkage()</code>函数对模型的<strong>R平方统计量</strong>做了k重交叉验证：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">library</span>(bootstrap)</span><br><span class="line">&gt; shrinkage &lt;- <span class="keyword">function</span>(fit, k = <span class="number">10</span>) &#123;</span><br><span class="line">    <span class="keyword">require</span>(bootstrap)</span><br><span class="line">    theta.fit &lt;- <span class="keyword">function</span>(x, y) &#123;</span><br><span class="line">        lsfit(x, y)</span><br><span class="line">    &#125;</span><br><span class="line">    theta.predict &lt;- <span class="keyword">function</span>(fit, x) &#123;</span><br><span class="line">        cbind(<span class="number">1</span>, x) %*% fit$coef</span><br><span class="line">    &#125;</span><br><span class="line">    x &lt;- fit$model[, <span class="number">2</span>:ncol(fit$model)]</span><br><span class="line">    y &lt;- fit$model[, <span class="number">1</span>]</span><br><span class="line">    results &lt;- crossval(x, y, theta.fit, theta.predict, ngroup = k)</span><br><span class="line">    r2 &lt;- cor(y, fit$fitted.values)^<span class="number">2</span></span><br><span class="line">    r2cv &lt;- cor(y, results$cv.fit)^<span class="number">2</span></span><br><span class="line">    cat(<span class="string">"Original R-square ="</span>, r2, <span class="string">"\n"</span>)</span><br><span class="line">    cat(k, <span class="string">"Fold Cross-Validated R-square ="</span>, r2cv, <span class="string">"\n"</span>)</span><br><span class="line">    cat(<span class="string">"Change ="</span>, r2 - r2cv, <span class="string">"\n"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&gt; states &lt;- as.data.frame(state.x77[, c(<span class="string">"Murder"</span>, <span class="string">"Population"</span>, <span class="string">"Illiteracy"</span>, <span class="string">"Income"</span>, <span class="string">"Frost"</span>)])</span><br><span class="line">&gt; fit &lt;- lm(Murder ~ Population + Income + Illiteracy + Frost, data = states)</span><br><span class="line">&gt; shrinkage(fit)</span><br><span class="line">Original R-square = <span class="number">0.5669502</span> </span><br><span class="line"><span class="number">10</span> Fold Cross-Validated R-square = <span class="number">0.3957056</span> </span><br><span class="line">Change = <span class="number">0.1712447</span></span><br></pre></td></tr></table></figure>
<p>可以看到，没做交叉验证之前，模型的R方过于乐观了(0.567)，而10重交叉验证显示，模型的R方只有0.396，相去甚远。</p>
<p><strong>交叉验证还能用来验证所选择的变量的效果</strong>，例如只用两个变量：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; fit2 &lt;- lm(Murder ~ Population + Illiteracy,data=states)</span><br><span class="line">&gt; shrinkage(fit2)</span><br><span class="line">Original R-square = <span class="number">0.5668327</span> </span><br><span class="line"><span class="number">10</span> Fold Cross-Validated R-square = <span class="number">0.5293426</span> </span><br><span class="line">Change = <span class="number">0.03749011</span></span><br></pre></td></tr></table></figure>
<p>可以看到模型2拟合的更好。</p>
<h2 id="相对重要性">相对重要性</h2>
<p>如果一个OSL模型有多个自变量，那么有时候你会想知道，哪个自变量对因变量的影响是最大的。这就是自变量的相对重要性。</p>
<p>目前有如下方法来计算自变量之间的相对重要性：</p>
<ol type="1">
<li><p>若自变量之间不相关，可以根据<strong>自变量与因变量的相关系数</strong>来排序。但大多数时候自变量之间或多或少都会有相关性。</p></li>
<li><p>除去第一种方法，最简单的则是比较<strong>标准化的回归系数</strong>，它表示当其他预测变量不变时，该自变量一个标准差的变化可引起的因变量的预期变化(以标准差单位度量)。在进行回归分析前，可用<code>scale()</code>函数将数据标准化为均值为0、标准差为1的数据集，这样模型中的系数即是标准化的回归系数：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; states &lt;- as.data.frame(state.x77[, c(<span class="string">"Murder"</span>, <span class="string">"Population"</span>, <span class="string">"Illiteracy"</span>, <span class="string">"Income"</span>, <span class="string">"Frost"</span>)])</span><br><span class="line">&gt; zstates &lt;- as.data.frame(scale(states))</span><br><span class="line">&gt; zfit &lt;- lm(Murder ~ Population + Income + Illiteracy + Frost, data = zstates)</span><br><span class="line">&gt; coef(zfit)</span><br><span class="line">  (Intercept)    Population        Income    Illiteracy         Frost </span><br><span class="line">-<span class="number">2.054026e-16</span>  <span class="number">2.705095e-01</span>  <span class="number">1.072372e-02</span>  <span class="number">6.840496e-01</span>  <span class="number">8.185407e-03</span></span><br></pre></td></tr></table></figure>
<p>可以看到Illiteracy的影响是最大的。</p></li>
<li><p><strong>相对权重</strong>(relative weight)是一种比较有前景的新方法，它是对所有可能子模型添加一个预 测变量引起的R平方平均增加量的一个近似值。暂时未找到该方法的第三方包，且得出的结果与第二种方法以及前面的分析略有偏差，这里不做描述。</p></li>
</ol>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>嘴强黑铁
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://www.zuiqiangiron.xyz/2020/04/25/R%E8%AF%AD%E8%A8%80%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/" title="R语言回归分析">https://www.zuiqiangiron.xyz/2020/04/25/R语言回归分析/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/R%E8%AF%AD%E8%A8%80/" rel="tag"># R语言</a>
              <a href="/tags/R%E8%AF%AD%E8%A8%80%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/" rel="tag"># R语言回归分析</a>
          </div>

        
  <div class="post-widgets">
    <div class="wp_rating">
      <div id="wpac-rating"></div>
    </div>
  </div>


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/04/24/R%E8%AF%AD%E8%A8%80%E5%9F%BA%E6%9C%AC%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/" rel="prev" title="R语言基本统计分析">
      <i class="fa fa-chevron-left"></i> R语言基本统计分析
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#思维导图"><span class="nav-number">1.</span> <span class="nav-text">思维导图</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#osl回归"><span class="nav-number">2.</span> <span class="nav-text">OSL回归</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#几种常用的osl回归模型"><span class="nav-number">2.1.</span> <span class="nav-text">几种常用的OSL回归模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#线性模型与非线性模型"><span class="nav-number">2.1.1.</span> <span class="nav-text">线性模型与非线性模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lm函数拟合回归模型"><span class="nav-number">2.2.</span> <span class="nav-text">lm()函数拟合回归模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简单线性回归"><span class="nav-number">2.2.1.</span> <span class="nav-text">简单线性回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多项式回归"><span class="nav-number">2.2.2.</span> <span class="nav-text">多项式回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#更方便的函数scatterplot"><span class="nav-number">2.2.3.</span> <span class="nav-text">更方便的函数：scatterplot()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#有交互项的多元线性回归"><span class="nav-number">2.2.4.</span> <span class="nav-text">有交互项的多元线性回归</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#回归诊断"><span class="nav-number">3.</span> <span class="nav-text">回归诊断</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#标准方法"><span class="nav-number">3.1.</span> <span class="nav-text">标准方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#改进的方法"><span class="nav-number">3.2.</span> <span class="nav-text">改进的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#检验正态性"><span class="nav-number">3.2.1.</span> <span class="nav-text">检验正态性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#检验误差的独立性"><span class="nav-number">3.2.2.</span> <span class="nav-text">检验误差的独立性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#检验线性"><span class="nav-number">3.2.3.</span> <span class="nav-text">检验线性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#检验同方差性"><span class="nav-number">3.2.4.</span> <span class="nav-text">检验同方差性</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ncvtest"><span class="nav-number">3.2.4.1.</span> <span class="nav-text">ncvTest()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#spreadlevelplot"><span class="nav-number">3.2.4.2.</span> <span class="nav-text">spreadLevelPlot()</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性模型假设的综合验证"><span class="nav-number">3.3.</span> <span class="nav-text">线性模型假设的综合验证</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多重共线性"><span class="nav-number">3.4.</span> <span class="nav-text">多重共线性</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#异常值分析"><span class="nav-number">4.</span> <span class="nav-text">异常值分析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#离群点"><span class="nav-number">4.1.</span> <span class="nav-text">离群点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#高杠杆点"><span class="nav-number">4.2.</span> <span class="nav-text">高杠杆点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#强影响点"><span class="nav-number">4.3.</span> <span class="nav-text">强影响点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#cook距离"><span class="nav-number">4.3.1.</span> <span class="nav-text">Cook距离</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#变量添加图"><span class="nav-number">4.3.2.</span> <span class="nav-text">变量添加图</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#综合分析异常值"><span class="nav-number">4.4.</span> <span class="nav-text">综合分析异常值</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#改进模型"><span class="nav-number">5.</span> <span class="nav-text">改进模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#删除观测点"><span class="nav-number">5.1.</span> <span class="nav-text">删除观测点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#变量变换"><span class="nav-number">5.2.</span> <span class="nav-text">变量变换</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#模型违反正态假设时"><span class="nav-number">5.2.1.</span> <span class="nav-text">模型违反正态假设时</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模型违反线性假设时"><span class="nav-number">5.2.2.</span> <span class="nav-text">模型违反线性假设时</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模型违反方差同性假设时"><span class="nav-number">5.2.3.</span> <span class="nav-text">模型违反方差同性假设时</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#增删变量"><span class="nav-number">5.3.</span> <span class="nav-text">增删变量</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#选择最佳回归模型"><span class="nav-number">6.</span> <span class="nav-text">选择“最佳”回归模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#r自带的模型比较方法"><span class="nav-number">6.1.</span> <span class="nav-text">R自带的模型比较方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#anova比较模型"><span class="nav-number">6.1.1.</span> <span class="nav-text">anova()比较模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#aic函数比较模型"><span class="nav-number">6.1.2.</span> <span class="nav-text">AIC()函数比较模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型的自变量选择"><span class="nav-number">6.2.</span> <span class="nav-text">模型的自变量选择</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#逐步回归"><span class="nav-number">6.2.1.</span> <span class="nav-text">逐步回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#全子集回归"><span class="nav-number">6.2.2.</span> <span class="nav-text">全子集回归</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#深层次分析"><span class="nav-number">7.</span> <span class="nav-text">深层次分析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#交叉验证"><span class="nav-number">7.1.</span> <span class="nav-text">交叉验证</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#相对重要性"><span class="nav-number">7.2.</span> <span class="nav-text">相对重要性</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="嘴强黑铁"
      src="/images/touxiang.jpg">
  <p class="site-author-name" itemprop="name">嘴强黑铁</p>
  <div class="site-description" itemprop="description">一顿操作猛如虎</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">72</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">24</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">73</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">嘴强黑铁</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.7.2
  </div>
  <div class="addthis_inline_share_toolbox">
    <script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5e731fa9610694ef" async="async"></script>
  </div>

        
<div class="busuanzi-count">
  <script pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.getAttribute('pjax') !== null) {
      script.setAttribute('pjax', '');
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  



  <script pjax>
  if (CONFIG.page.isPost) {
    wpac_init = window.wpac_init || [];
    wpac_init.push({
      widget: 'Rating',
      id    : 24025,
      el    : 'wpac-rating',
      color : 'fc6423'
    });
    (function() {
      if ('WIDGETPACK_LOADED' in window) return;
      WIDGETPACK_LOADED = true;
      var mc = document.createElement('script');
      mc.type = 'text/javascript';
      mc.async = true;
      mc.src = '//embed.widgetpack.com/widget.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling);
    })();
  }
  </script>

  
<script src="/js/local-search.js"></script>













    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://zuiqiangiron.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://www.zuiqiangiron.xyz/2020/04/25/R%E8%AF%AD%E8%A8%80%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/";
    this.page.identifier = "2020/04/25/R语言回归分析/";
    this.page.title = "R语言回归分析";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://zuiqiangiron.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

    </div>
</body>
</html>
